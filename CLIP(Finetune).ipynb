{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hmin27/2023_DL_Clip/blob/main/CLIP(Finetune).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx2hKkBgzUkz"
      },
      "source": [
        "# CLIP Fine tuning\n",
        "- Food image classification\n",
        "- Baseline of Fine Tuned CLIP model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UU5QbpXrDWUl"
      },
      "outputs": [],
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wc7W2ooyDcLZ",
        "outputId": "a5fddcb7-ee1d-47bb-edff-82e8064a21da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import clip\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "%matplotlib inline\n",
        "BATCH_SIZE = 32\n",
        "EPOCH = 3\n",
        "LR = 1e-5\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p52DoKG6DmVA"
      },
      "source": [
        "# Prepare the Model and Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnmtrDVrE9xn",
        "outputId": "dc416d80-ee91-4fbf-b868-8f047c882bb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4faeiH7QkCV",
        "outputId": "a58d6afb-38d8-4d6c-b526-6c8887ac6054"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:05<00:00, 66.6MiB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7883636a5910>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
        "model = model.to(torch.float32)\n",
        "\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79slliwKt9HA"
      },
      "outputs": [],
      "source": [
        "# Creating image path, text list\n",
        "import pandas as pd\n",
        "\n",
        "data_folder = '/content/drive/MyDrive/Study/DL_CLIP/Food'\n",
        "\n",
        "image_paths = []\n",
        "text_descriptions = []\n",
        "class_folders = os.listdir(data_folder)\n",
        "\n",
        "for class_folder in class_folders:\n",
        "    class_folder_path = os.path.join(data_folder, class_folder)\n",
        "    image_files = os.listdir(class_folder_path)\n",
        "\n",
        "    for image_file in image_files:\n",
        "        image_path = os.path.join(class_folder_path, image_file)\n",
        "        image_paths.append(image_path)\n",
        "\n",
        "        # Create text description using class label\n",
        "        text_description = f\"a photo of {class_folder.replace('_',' ')}\"\n",
        "        text_descriptions.append(text_description)\n",
        "\n",
        "print(text_descriptions)\n",
        "len(text_descriptions)  # 21091개\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rmQGe_hPek8Z"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, image_paths, text_descriptions, preprocess):\n",
        "        self.image_paths = image_paths\n",
        "        self.text_descriptions = text_descriptions\n",
        "        self.preprocess = preprocess\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx])\n",
        "        image = self.preprocess(image)\n",
        "        text = clip.tokenize(self.text_descriptions[idx])\n",
        "        return image, text\n",
        "\n",
        "\n",
        "dataset = MyDataset(image_paths, text_descriptions, preprocess)\n",
        "\n",
        "# train : validation : test = 7:2:1\n",
        "total_size = len(dataset)\n",
        "train_size = int(0.7 * total_size)\n",
        "val_size = int(0.2 * total_size)\n",
        "test_size = total_size - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "\n",
        "trainloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "testloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztG9Tqz-v1vs",
        "outputId": "ccbcabe2-cb55-4a6f-8e1e-ab9f47d59c6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image Path: torch.Size([3, 224, 224])\n",
            "Text Description: torch.Size([1, 77])\n"
          ]
        }
      ],
      "source": [
        "from numpy.lib import shape_base\n",
        "for batch in trainloader:\n",
        "    images, texts = batch\n",
        "    # Print the first batch\n",
        "    print(\"Image Path:\", images[0].shape)\n",
        "    print(\"Text Description:\", texts[0].shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fL2zwBDdNvZw"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KlgzBPDONu3x"
      },
      "outputs": [],
      "source": [
        "def convert_models_to_fp32(model):\n",
        "    for p in model.parameters():\n",
        "        p.data = p.data.float()\n",
        "        p.grad.data = p.grad.data.float()\n",
        "\n",
        "loss_img = nn.CrossEntropyLoss()\n",
        "loss_txt = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, len(trainloader)*EPOCH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3-uwItMMhlS"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def compute_accuracy(logits, ground_truth):\n",
        "    _, predicted = logits.max(1)\n",
        "    total = ground_truth.size(0)\n",
        "    correct = (predicted == ground_truth).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "for epoch in range(EPOCH):\n",
        "    print(f\"Epoch: {epoch+1}\")\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    train_total, train_correct = 0, 0\n",
        "    pbar = tqdm(trainloader, total=len(trainloader))\n",
        "    for batch in pbar:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        images, texts = batch\n",
        "        texts = texts.squeeze(1)\n",
        "        images = images.to(device)\n",
        "        texts = texts.to(device)\n",
        "\n",
        "        logits_per_image, logits_per_text = model(images, texts)\n",
        "\n",
        "        # Compute loss\n",
        "        actual_batch_size = images.size(0)\n",
        "        ground_truth = torch.arange(actual_batch_size).to(device)\n",
        "        total_loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2\n",
        "\n",
        "        # Compute train accuracy\n",
        "        train_correct += (logits_per_image.argmax(dim=1) == ground_truth).float().sum().item()\n",
        "        train_total += images.size(0)\n",
        "\n",
        "        total_loss.backward()\n",
        "\n",
        "        if device == \"cpu\":\n",
        "            optimizer.step()\n",
        "        else :\n",
        "            convert_models_to_fp32(model)\n",
        "            optimizer.step()\n",
        "            clip.model.convert_weights(model)\n",
        "\n",
        "        train_accuracy = 100 * train_correct / train_total\n",
        "        pbar.set_description(f\"Epoch {epoch+1}/{EPOCH}, Loss: {total_loss.item():.4f}, Train Acc: {train_accuracy:.2f}%\")\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_total, val_correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch in valloader:\n",
        "            images, texts = batch\n",
        "            texts = texts.squeeze(1)\n",
        "            images = images.to(device)\n",
        "            texts = texts.to(device)\n",
        "\n",
        "            logits_per_image, _ = model(images, texts)\n",
        "\n",
        "            actual_batch_size = logits_per_image.size(0)\n",
        "            ground_truth = torch.arange(actual_batch_size).to(device)\n",
        "            # ground_truth = torch.arange(BATCH_SIZE).to(device)\n",
        "\n",
        "            val_correct += (logits_per_image.argmax(dim=1) == ground_truth).float().sum().item()\n",
        "            val_total += images.size(0)\n",
        "\n",
        "    val_accuracy = 100 * val_correct / val_total\n",
        "    print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcveGi9uSziV"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'CLIP_v1.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}